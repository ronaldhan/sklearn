<!DOCTYPE html>
<html class="theme theme-white">
<head>
<meta charset="utf-8">
<title>SciKit-Learn PCA</title>
<link href="https://www.zybuluo.com/static/assets/template-theme-white.css" rel="stylesheet" media="screen">
</head>
<body class="theme theme-white">
<div id="wmd-preview" class="wmd-preview wmd-preview-full-reader"><div class="md-section-divider"></div><div class="md-section-divider"></div><h1 id="scikit-learn-pca" data-anchor-id="8rfd">SciKit-Learn PCA</h1><p data-anchor-id="6s4n"><code>SciKit-Learn</code> <code>PCA</code> <code>主成分分析</code><a href="../index.html" style="float:right"><strong>Home</strong></a></p><hr><p data-anchor-id="cukh">主成分分析是一种常用的降维方法，能够找出数据中具有最大方差的方向，非常适合于探索数据特性和数据可视化。</p><div class="md-section-divider"></div><pre style="" data-anchor-id="lgkb" class="prettyprint linenums prettyprinted"><ol class="linenums"><li class="L0"><code class="language-python"><span class="pln">np</span><span class="pun">.</span><span class="pln">random</span><span class="pun">.</span><span class="pln">seed</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span></code></li><li class="L1"><code class="language-python"><span class="pln">X </span><span class="pun">=</span><span class="pln"> np</span><span class="pun">.</span><span class="pln">dot</span><span class="pun">(</span><span class="pln">np</span><span class="pun">.</span><span class="pln">random</span><span class="pun">.</span><span class="pln">random</span><span class="pun">(</span><span class="pln">size</span><span class="pun">=(</span><span class="lit">2</span><span class="pun">,</span><span class="pln"> </span><span class="lit">2</span><span class="pun">)),</span><span class="pln"> np</span><span class="pun">.</span><span class="pln">random</span><span class="pun">.</span><span class="pln">normal</span><span class="pun">(</span><span class="pln">size</span><span class="pun">=(</span><span class="lit">2</span><span class="pun">,</span><span class="pln"> </span><span class="lit">200</span><span class="pun">))).</span><span class="pln">T</span></code></li><li class="L2"><code class="language-python"><span class="pln">plt</span><span class="pun">.</span><span class="pln">plot</span><span class="pun">(</span><span class="pln">X</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">0</span><span class="pun">],</span><span class="pln"> X</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">1</span><span class="pun">],</span><span class="pln"> </span><span class="str">'o'</span><span class="pun">)</span></code></li><li class="L3"><code class="language-python"><span class="pln">plt</span><span class="pun">.</span><span class="pln">axis</span><span class="pun">(</span><span class="str">'equal'</span><span class="pun">);</span></code></li></ol></pre><p data-anchor-id="ck9j"><img src="http://static.zybuluo.com/ronaldhan/fklcvxfi32m8j0bifmxwf5qs/output_5_0.png" alt="output_5_0.png-7.7kB" title=""> <br>
可以观察到数据中存在明显的方向，PCA就是要找到这些占据主导的轴的方向，并且衡量每个方向对于描述数据分布的贡献。</p><div class="md-section-divider"></div><pre style="" data-anchor-id="cc6c" class="prettyprint linenums prettyprinted"><ol class="linenums"><li class="L0"><code class="language-python"><span class="kwd">from</span><span class="pln"> sklearn</span><span class="pun">.</span><span class="pln">decomposition </span><span class="kwd">import</span><span class="pln"> PCA</span></code></li><li class="L1"><code class="language-python"><span class="pln">pca </span><span class="pun">=</span><span class="pln"> PCA</span><span class="pun">(</span><span class="pln">n_components</span><span class="pun">=</span><span class="lit">2</span><span class="pun">)</span></code></li><li class="L2"><code class="language-python"><span class="pln">pca</span><span class="pun">.</span><span class="pln">fit</span><span class="pun">(</span><span class="pln">X</span><span class="pun">)</span></code></li><li class="L3"><code class="language-python"><span class="kwd">print</span><span class="pun">(</span><span class="pln">pca</span><span class="pun">.</span><span class="pln">explained_variance_</span><span class="pun">)</span></code></li><li class="L4"><code class="language-python"><span class="kwd">print</span><span class="pun">(</span><span class="pln">pca</span><span class="pun">.</span><span class="pln">components_</span><span class="pun">)</span></code></li><li class="L5"><code class="language-python"></code></li><li class="L6"><code class="language-python"><span class="pun">&gt;&gt;&gt;</span><span class="pln"> </span><span class="pun">[</span><span class="pln"> </span><span class="lit">0.75871884</span><span class="pln">  </span><span class="lit">0.01838551</span><span class="pun">]</span></code></li><li class="L7"><code class="language-python"><span class="pun">[[</span><span class="pln"> </span><span class="lit">0.94446029</span><span class="pln">  </span><span class="lit">0.32862557</span><span class="pun">]</span></code></li><li class="L8"><code class="language-python"><span class="pln"> </span><span class="pun">[</span><span class="pln"> </span><span class="lit">0.32862557</span><span class="pln"> </span><span class="pun">-</span><span class="lit">0.94446029</span><span class="pun">]]</span></code></li></ol></pre><div class="md-section-divider"></div><pre style="" data-anchor-id="isw3" class="prettyprint linenums prettyprinted"><ol class="linenums"><li class="L0"><code class="language-python"><span class="pln">plt</span><span class="pun">.</span><span class="pln">plot</span><span class="pun">(</span><span class="pln">X</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">0</span><span class="pun">],</span><span class="pln"> X</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">1</span><span class="pun">],</span><span class="pln"> </span><span class="str">'o'</span><span class="pun">,</span><span class="pln"> alpha</span><span class="pun">=</span><span class="lit">0.5</span><span class="pun">)</span></code></li><li class="L1"><code class="language-python"><span class="kwd">for</span><span class="pln"> length</span><span class="pun">,</span><span class="pln"> vector </span><span class="kwd">in</span><span class="pln"> zip</span><span class="pun">(</span><span class="pln">pca</span><span class="pun">.</span><span class="pln">explained_variance_ratio_</span><span class="pun">,</span><span class="pln"> pca</span><span class="pun">.</span><span class="pln">components_</span><span class="pun">):</span></code></li><li class="L2"><code class="language-python"><span class="pln">    v </span><span class="pun">=</span><span class="pln"> vector </span><span class="pun">*</span><span class="pln"> </span><span class="lit">3</span><span class="pln"> </span><span class="pun">*</span><span class="pln"> np</span><span class="pun">.</span><span class="pln">sqrt</span><span class="pun">(</span><span class="pln">length</span><span class="pun">)</span></code></li><li class="L3"><code class="language-python"><span class="pln">    plt</span><span class="pun">.</span><span class="pln">plot</span><span class="pun">([</span><span class="lit">0</span><span class="pun">,</span><span class="pln"> v</span><span class="pun">[</span><span class="lit">0</span><span class="pun">]],</span><span class="pln"> </span><span class="pun">[</span><span class="lit">0</span><span class="pun">,</span><span class="pln"> v</span><span class="pun">[</span><span class="lit">1</span><span class="pun">]],</span><span class="pln"> </span><span class="str">'-k'</span><span class="pun">,</span><span class="pln"> lw</span><span class="pun">=</span><span class="lit">3</span><span class="pun">)</span></code></li><li class="L4"><code class="language-python"><span class="pln">plt</span><span class="pun">.</span><span class="pln">axis</span><span class="pun">(</span><span class="str">'equal'</span><span class="pun">);</span></code></li></ol></pre><p data-anchor-id="ncut"><img src="http://static.zybuluo.com/ronaldhan/i6jwv46i5su82g9zqfnpy0ba/output_8_0.png" alt="output_8_0.png-13.7kB" title=""> <br>
注意到有两个方向，并且每个方向上线的长度不相同，长度越长表明该方向对于解释数据分布更重要。有时，可以将第二主成分忽略掉而不损失多少信息。将我们的PCA解释的方差设置为95%。</p><div class="md-section-divider"></div><pre style="" data-anchor-id="5w9g" class="prettyprint linenums prettyprinted"><ol class="linenums"><li class="L0"><code class="language-python"><span class="pln">clf </span><span class="pun">=</span><span class="pln"> PCA</span><span class="pun">(</span><span class="lit">0.95</span><span class="pun">)</span><span class="pln"> </span><span class="com"># keep 95% of variance</span></code></li><li class="L1"><code class="language-python"><span class="pln">X_trans </span><span class="pun">=</span><span class="pln"> clf</span><span class="pun">.</span><span class="pln">fit_transform</span><span class="pun">(</span><span class="pln">X</span><span class="pun">)</span></code></li><li class="L2"><code class="language-python"><span class="kwd">print</span><span class="pun">(</span><span class="pln">X</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">)</span></code></li><li class="L3"><code class="language-python"><span class="kwd">print</span><span class="pun">(</span><span class="pln">X_trans</span><span class="pun">.</span><span class="pln">shape</span><span class="pun">)</span></code></li><li class="L4"><code class="language-python"></code></li><li class="L5"><code class="language-python"><span class="pun">&gt;&gt;&gt;</span><span class="pln"> </span><span class="pun">(</span><span class="lit">200</span><span class="pun">,</span><span class="pln"> </span><span class="lit">2</span><span class="pun">)</span></code></li><li class="L6"><code class="language-python"><span class="pun">(</span><span class="lit">200</span><span class="pun">,</span><span class="pln"> </span><span class="lit">1</span><span class="pun">)</span></code></li></ol></pre><p data-anchor-id="bzs6">从输出结果中可以看到X的维度只剩下1了，减少了50%，我们再观察发生的变化。</p><div class="md-section-divider"></div><pre style="" data-anchor-id="as9a" class="prettyprint linenums prettyprinted"><ol class="linenums"><li class="L0"><code class="language-python"><span class="pln">X_new </span><span class="pun">=</span><span class="pln"> clf</span><span class="pun">.</span><span class="pln">inverse_transform</span><span class="pun">(</span><span class="pln">X_trans</span><span class="pun">)</span></code></li><li class="L1"><code class="language-python"><span class="pln">plt</span><span class="pun">.</span><span class="pln">plot</span><span class="pun">(</span><span class="pln">X</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">0</span><span class="pun">],</span><span class="pln"> X</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">1</span><span class="pun">],</span><span class="pln"> </span><span class="str">'o'</span><span class="pun">,</span><span class="pln"> alpha</span><span class="pun">=</span><span class="lit">0.2</span><span class="pun">)</span></code></li><li class="L2"><code class="language-python"><span class="pln">plt</span><span class="pun">.</span><span class="pln">plot</span><span class="pun">(</span><span class="pln">X_new</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">0</span><span class="pun">],</span><span class="pln"> X_new</span><span class="pun">[:,</span><span class="pln"> </span><span class="lit">1</span><span class="pun">],</span><span class="pln"> </span><span class="str">'ob'</span><span class="pun">,</span><span class="pln"> alpha</span><span class="pun">=</span><span class="lit">0.8</span><span class="pun">)</span></code></li><li class="L3"><code class="language-python"><span class="pln">plt</span><span class="pun">.</span><span class="pln">axis</span><span class="pun">(</span><span class="str">'equal'</span><span class="pun">);</span></code></li></ol></pre><p data-anchor-id="8c7m"><img src="http://static.zybuluo.com/ronaldhan/594ypsgvj4pz1rzg09m6nrta/output_13_0.png" alt="output_13_0.png-13.2kB" title=""> <br>
图中颜色较浅的是原始数据点，蓝色的是经过投影后的数据点，仅仅损失5%的方差就将数据压缩了50%。</p></div>
</body>
</html>